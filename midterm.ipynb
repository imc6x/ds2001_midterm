{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "This notebook uses the PyMongo database connectivity library to connect to MySQL databases; therefore, you must have first installed that libary into your python environment by executing the following command in a Terminal window.\n",
    "\n",
    "- `python -m pip install pymongo[srv]`\n",
    "\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import \n",
    "import certifi\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.38\n",
      "Running PyMongo Version: 4.11.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MongoDB Server, the MySQL Server & Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_args_source = {\n",
    "    \"uid\" : \"root\",\n",
    "    \"pwd\" : \"chubby100\",\n",
    "    \"hostname\" : \"localhost\",\n",
    "    \"dbname\" : \"adventureworks\"\n",
    "}\n",
    "\n",
    "mysql_args_dw = {\n",
    "    \"uid\": \"root\",\n",
    "    \"pwd\": \"chubby100\",\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"dbname\": \"adventureworks_dw\"\n",
    "}\n",
    "\n",
    "# The 'cluster_location' must either be \"atlas\" or \"local\".\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"\",\n",
    "    \"password\" : \"password\",\n",
    "    \"cluster_name\" : \"cluster_name\",\n",
    "    \"cluster_subnet\" : \"xxxxx\",\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"db_name\" : \"adventureworks_mdb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mysql+pymysql://{mysql_args_dw['uid']}:{mysql_args_dw['pwd']}@{mysql_args_dw['hostname']}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "connection = sqlEngine.connect()\n",
    "\n",
    "connection.execute(text(f\"DROP DATABASE IF EXISTS `{mysql_args_dw}`;\"))\n",
    "connection.execute(text(\"CREATE DATABASE IF NOT EXISTS adventureworks_dw;\"))\n",
    "connection.execute(text(f\"USE {mysql_args_dw};\"))\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(text(sql_query), connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        connection.execute(text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\"))\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name, data_directory, json_files):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of dim_date: In order for this database to correctly work, you must run the dim_date_creation.sql file located in the scripts directory on the newly created database, adventureworks_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data from source to be loaded into csv, json, and fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product data to be used for MongoDB\n",
    "\n",
    "sql_products = \"\"\"\n",
    "SELECT \n",
    "    p.ProductID,\n",
    "    p.Name,\n",
    "    p.ProductNumber,\n",
    "    p.Color,\n",
    "    p.ListPrice,\n",
    "    p.StandardCost,\n",
    "    pc.Name AS ProductCategory,\n",
    "    psc.Name AS ProductSubcategory\n",
    "FROM \n",
    "    product AS p\n",
    "LEFT JOIN \n",
    "    productsubcategory AS psc ON p.ProductSubcategoryID = psc.ProductSubcategoryID\n",
    "LEFT JOIN \n",
    "    productcategory AS pc ON psc.ProductCategoryID = pc.ProductCategoryID\n",
    "\"\"\"\n",
    "df_products = get_sql_dataframe(sql_products, **mysql_args_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer data to be used for CSV\n",
    "\n",
    "sql_customers = \"\"\"\n",
    "SELECT \n",
    "    c.CustomerID,\n",
    "    c.AccountNumber,\n",
    "    c.CustomerType,\n",
    "    co.FirstName,\n",
    "    co.LastName,\n",
    "    co.EmailAddress,\n",
    "    a.City,\n",
    "    sp.Name AS StateProvince\n",
    "FROM \n",
    "    customer AS c\n",
    "LEFT JOIN \n",
    "    individual AS i ON c.CustomerID = i.CustomerID\n",
    "LEFT JOIN \n",
    "    contact AS co ON i.ContactID = co.ContactID\n",
    "LEFT JOIN \n",
    "    customeraddress AS ca ON c.CustomerID = ca.CustomerID\n",
    "LEFT JOIN \n",
    "    address AS a ON ca.AddressID = a.AddressID\n",
    "LEFT JOIN \n",
    "    stateprovince AS sp ON a.StateProvinceID = sp.StateProvinceID\n",
    "WHERE\n",
    "    co.FirstName IS NOT NULL\n",
    "\"\"\"\n",
    "df_customers = get_sql_dataframe(sql_customers, **mysql_args_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales data to be used for fact table\n",
    "\n",
    "sql_sales = \"\"\"\n",
    "SELECT \n",
    "    soh.SalesOrderID,\n",
    "    soh.OrderDate,\n",
    "    soh.CustomerID,\n",
    "    sod.ProductID,\n",
    "    sod.OrderQty,\n",
    "    sod.UnitPrice,\n",
    "    sod.UnitPriceDiscount,\n",
    "    sod.LineTotal\n",
    "FROM \n",
    "    salesorderheader AS soh\n",
    "JOIN \n",
    "    salesorderdetail AS sod ON soh.SalesOrderID = sod.SalesOrderID\n",
    "\"\"\"\n",
    "df_sales = get_sql_dataframe(sql_sales, **mysql_args_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory \n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save products to a csv\n",
    "products_csv_path = os.path.join(data_dir, 'products.csv')\n",
    "df_products.to_csv(products_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save customer data to JSON\n",
    "# Reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html#pandas-dataframe-to-json \n",
    "\n",
    "customers_json_path = os.path.join('data', 'customers.json')\n",
    "json_str = df_customers.to_json(orient='records')\n",
    "parsed = json.loads(json_str)\n",
    "with open(customers_json_path, 'w') as f:\n",
    "    json.dump(parsed, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customer data into MongoDB\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "json_files = {\"customers\": \"customers.json\"}\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV Data\n",
    "\n",
    "df_products_from_csv = pd.read_csv(products_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MongoDB data\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "query = {} \n",
    "df_customers_from_mongo = get_mongo_dataframe(client, mongodb_args[\"db_name\"], \"customers\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load dimension tables for data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['product_key', 'ProductID', 'Name', 'ProductNumber', 'Color',\n",
      "       'ListPrice', 'StandardCost', 'ProductCategory', 'ProductSubcategory'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Product dimensions \n",
    "\n",
    "df_dim_products = df_products_from_csv.copy()\n",
    "df_dim_products.insert(0, \"product_key\", range(1, df_dim_products.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer dimensions \n",
    "\n",
    "df_dim_customers = df_customers_from_mongo.copy()\n",
    "df_dim_customers.insert(0, \"customer_key\", range(1, df_dim_customers.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dimension tables into data warehouse\n",
    "\n",
    "set_dataframe(df_dim_products, 'dim_product', 'product_key', 'insert', **mysql_args_dw)\n",
    "set_dataframe(df_dim_customers, 'dim_customer', 'customer_key', 'insert', **mysql_args_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data to match dim_date\n",
    "\n",
    "# Copy data to not affect original https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html \n",
    "df_fact_sales = df_sales.copy()\n",
    "\n",
    "df_fact_sales['OrderDate'] = pd.to_datetime(df_fact_sales['OrderDate'])\n",
    "df_fact_sales['order_date_key'] = df_fact_sales['OrderDate'].dt.strftime('%Y%m%d').astype(int) #to match dim_date https://stackoverflow.com/questions/70105247/convert-y-m-d-hms-format-to-int-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up dataframes for products and customers\n",
    "\n",
    "df_product_lookup = df_dim_products[['product_key', 'ProductID']]\n",
    "df_customer_lookup = df_dim_customers[['customer_key', 'CustomerID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining in order to get surrogate keys for the fact table\n",
    "\n",
    "df_fact_sales = pd.merge(df_fact_sales, df_product_lookup, on='ProductID', how='left')\n",
    "df_fact_sales = pd.merge(df_fact_sales, df_customer_lookup, on='CustomerID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sales = df_fact_sales[[ \n",
    "    'ProductID', 'product_key', 'CustomerID', 'customer_key', 'order_date_key',\n",
    "    'SalesOrderID', 'OrderQty', 'UnitPrice', 'UnitPriceDiscount', 'LineTotal'\n",
    "]]\n",
    "df_fact_sales.insert(0, \"sales_key\", range(1, df_fact_sales.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fact table\n",
    "\n",
    "set_dataframe(df_fact_sales, 'fact_sales', 'sales_key', 'insert', **mysql_args_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     month_name              ProductName ProductCategory  TotalQuantity  \\\n",
      "0          June  Mountain-200 Silver, 38           Bikes           84.0   \n",
      "1           May  Mountain-200 Silver, 46           Bikes           79.0   \n",
      "2          June   Mountain-200 Black, 42           Bikes           79.0   \n",
      "3           May   Mountain-200 Black, 38           Bikes           77.0   \n",
      "4          June  Mountain-200 Silver, 42           Bikes           75.0   \n",
      "...         ...                      ...             ...            ...   \n",
      "1544   November          Racing Socks, L        Clothing           20.0   \n",
      "1545   December          Racing Socks, L        Clothing           20.0   \n",
      "1546       July          Racing Socks, M        Clothing           19.0   \n",
      "1547   February          Racing Socks, L        Clothing           19.0   \n",
      "1548       July          Racing Socks, L        Clothing           12.0   \n",
      "\n",
      "       TotalSales  \n",
      "0     189410.6112  \n",
      "1     178059.2316  \n",
      "2     176386.3740  \n",
      "3     170812.8268  \n",
      "4     168530.7012  \n",
      "...           ...  \n",
      "1544     179.8000  \n",
      "1545     179.8000  \n",
      "1546     170.8100  \n",
      "1547     170.8100  \n",
      "1548     107.8800  \n",
      "\n",
      "[1549 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "validation_sql = \"\"\"\n",
    "SELECT \n",
    "    d.month_name,\n",
    "    p.Name AS ProductName,\n",
    "    p.ProductCategory,\n",
    "    SUM(s.OrderQty) AS TotalQuantity,\n",
    "    SUM(s.LineTotal) AS TotalSales\n",
    "FROM \n",
    "    adventureworks_dw.fact_sales AS s\n",
    "JOIN \n",
    "    adventureworks_dw.dim_product AS p ON s.product_key = p.product_key\n",
    "JOIN \n",
    "    adventureworks_dw.dim_customer AS c ON s.customer_key = c.customer_key\n",
    "JOIN \n",
    "    adventureworks_dw.dim_date AS d ON s.order_date_key = d.date_key\n",
    "GROUP BY \n",
    "    d.month_name, p.Name, p.ProductCategory\n",
    "ORDER BY \n",
    "    TotalSales DESC\n",
    "\"\"\"\n",
    "result_df = get_sql_dataframe(validation_sql, **mysql_args_dw)\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
